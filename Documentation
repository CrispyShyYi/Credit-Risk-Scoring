# Machine Learning Pipeline Documentation for Binary Classification

This documentation outlines the techniques employed in a machine learning pipeline designed for a binary classification task. The pipeline is divided into two main phases: **data preprocessing** and **model training and evaluation**. Multiple scripts contribute to this process, utilizing various methods to ensure robust data preparation and model performance.

## 1. Data Preprocessing

The data preprocessing phase prepares the raw data (`data_devsample.csv` (59.4 MB) for training and `data_to_score.csv` (73.7 MB) for prediction) for modeling by addressing missing values, encoding variables, and standardizing features. The following techniques were applied:

### Handling Missing Values
- **Categorical Columns:**
  - Missing values were filled with the string `"Missing"` to preserve information and enable subsequent encoding.
- **Numerical Columns:**
  - Missing values were imputed using **K-Nearest Neighbors (KNN) Imputation** with 5 neighbors (`n_neighbors=5`) after standardization to ensure accurate neighbor-based imputation.
- **Target Variable:**
  - Rows with missing values in the `'TARGET'` column were identified and, as a standard practice, assumed to be excluded from training to ensure valid supervised learning.

### Encoding Categorical Variables
- **One-Hot Encoding:**
  - Categorical columns were transformed into binary features using one-hot encoding with `drop_first=True` to avoid multicollinearity, expanding the feature set based on unique categories.

### Standardization
- **StandardScaler:**
  - Numerical columns were standardized using `StandardScaler` before KNN imputation to normalize the data, ensuring consistent scales across features. After imputation, the data was inverse-transformed to its original scale for model training.

### Feature Selection
- **Missing Value Threshold:**
  - Columns with more than 50% missing values were dropped to maintain data quality and reduce noise.
- **Manual Column Exclusion:**
  - Specific columns (`'TARGET'`, `'SK_ID_CURR'`, `'TIME'`, `'BASE'`, `'DAY'`) were removed from the training features, and a subset (`'SK_ID_CURR'`, `'TIME'`, `'BASE'`, `'DAY'`) from the test features, as they were deemed irrelevant or identifiers.

### Data Alignment
- **Column Reindexing:**
  - The test dataset was aligned with the training dataset’s feature set post-one-hot encoding using `reindex`, filling missing columns with zeros to ensure consistency.

### Infinity Handling
- **Replacement of Infinite Values:**
  - Infinite values (both positive and negative) in numerical columns were replaced with `NaN` before imputation to prevent computational errors.

### Output
- Processed training data (with the target re-added) and test data were saved as `"processed_train.csv"` and `"processed_test.csv"`, respectively, for use in subsequent modeling steps.

---

## 2. Model Training and Evaluation

Three machine learning models—**LightGBM**, **Random Forest**, and **XGBoost**—were trained and evaluated using the preprocessed data. The following techniques were employed across these models:

### Models Utilized
- **LightGBM:**
  - A gradient boosting framework optimized for speed and scalability.
- **Random Forest:**
  - An ensemble method using decision trees for robust classification.
- **XGBoost:**
  - Another gradient boosting framework known for high performance and flexibility.

### Hyperparameter Tuning
- **Optuna:**
  - An automated hyperparameter optimization framework was used to tune model parameters for all three models, maximizing the **ROC AUC** score.
  - **LightGBM Parameters:**
    - Tuned: `learning_rate`, `num_leaves`, `max_depth`, `min_child_samples`, `subsample`, `colsample_bytree`, `reg_alpha`, `reg_lambda`.
    - Fixed: `objective='binary'`, `metric='auc'`, `boosting_type='gbdt'`, `scale_pos_weight`.
    - Trials: 50.
  - **Random Forest Parameters:**
    - Tuned: `n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`, `max_features`.
    - Fixed: `random_state=42`, `n_jobs=-1`.
    - Trials: 50 (full model), 30 (simpler model with top 50 features).
  - **XGBoost Parameters:**
    - Tuned: `learning_rate`, `max_depth`, `min_child_weight`, `subsample`, `colsample_bytree`, `gamma`, `reg_alpha`, `reg_lambda`.
    - Fixed: `objective='binary:logistic'`, `tree_method='hist'`, `predictor='cpu_predictor'`, `scale_pos_weight`, `eval_metric='auc'`.
    - Trials: 50.

### Handling Class Imbalance
- **Scale Pos Weight:**
  - For **LightGBM** and **XGBoost**, the `scale_pos_weight` parameter was calculated as the ratio of negative to positive samples.
- **Random Forest:**
  - No explicit parameter was set, relying on the model’s inherent ability to handle imbalance through ensemble averaging.

### Early Stopping
- **LightGBM and XGBoost:**
  - Early stopping was implemented to prevent overfitting, halting training when the validation set’s performance (ROC AUC) stopped improving after a set number of rounds.

### Evaluation Metric
- **ROC AUC:**
  - The Area Under the Receiver Operating Characteristic Curve (ROC AUC) was the primary metric for model evaluation and hyperparameter tuning, suitable for imbalanced datasets.

### Threshold Optimization
- **Youden’s J Statistic:**
  - The optimal decision threshold was determined by maximizing `TPR - FPR` (True Positive Rate minus False Positive Rate) on the validation set.

### Feature Importance
- **Random Forest:**
  - Feature importances were calculated (`feature_importances_`), and a simpler model was trained using the top 50 most important features to explore dimensionality reduction.

### Cross-Validation
- **Random Forest:**
  - 5-fold stratified cross-validation was used during hyperparameter tuning to ensure robust performance estimates.
- **XGBoost:**
  - 5-fold stratified k-fold cross-validation was employed within the Optuna objective to evaluate model performance across multiple splits.

### Visualization
- **Optuna Visualizations (Random Forest):**
  - Optimization history and parameter importances were visualized to analyze the tuning process.

### Data Splitting
- **Train-Test Split:**
  - The training data was split into training and validation sets (80-20 split) with stratification to maintain class distribution.

### Final Predictions
- **Output Format:**
  - Predictions included both probabilities (`TARGET_PROB`) and binary classifications (`TARGET`) based on the optimal threshold.
  - Results were saved to CSV files:
    - `"predicted_results_LightGBM.csv"`
    - `"predicted_results_RF.csv"` (full model)
    - `"predicted_results_top50_RF.csv"` (simpler model)
    - `"predicted_results_XGBoost.csv"`.
- **Identifier Restoration:**
  - The `'SK_ID_CURR'` column from the original test data was included in the submission files.

---

## 3. Additional Techniques

### Feature Name Cleaning
- **LightGBM:**
  - Feature names were cleaned to remove special characters (`[^\w]`) to comply with LightGBM’s requirements.

### Data Matrix Conversion
- **XGBoost:**
  - Data was converted to `DMatrix` format for efficient processing.

---

## 4. Extended Approaches

### Parallel Training of Simpler XGBoost Models
After training a full XGBoost model, an additional approach involved selecting smaller subsets of the most important features (e.g., top 25, 50, 75, and 100) for training simpler models. These simplified models were trained in parallel, allowing for quicker experimentation with varying numbers of features to compare performance, complexity, and potential speed gains.

### SMOTEENN + PCA
A further method combined SMOTEENN (an oversampling and undersampling technique) with Principal Component Analysis (PCA), retaining 99% of variance. This approach directly addressed class imbalance and high-dimensional data, leading to an **AUROC exceeding 0.9** in some experiments. By combining sophisticated sampling methods with dimensionality reduction, this strategy can notably enhance predictive performance for imbalanced datasets.

---

## Summary

This pipeline integrates a variety of techniques to preprocess data effectively and train robust machine learning models:
- **Preprocessing:** Handles missing values, encodes categorical variables, standardizes numerical features, and aligns datasets.
- **Modeling:** Employs LightGBM, Random Forest, and XGBoost with optimized hyperparameters, addressing class imbalance and overfitting.
- **Evaluation:** Uses ROC AUC and threshold optimization for performance assessment, supplemented by cross-validation and feature importance analysis.
- **Extended Approaches:** Explores additional strategies such as parallelized training of feature-subset XGBoost models and combining SMOTEENN with PCA for advanced class imbalance handling and dimensionality reduction.

The resulting predictions are well-suited for binary classification tasks, with outputs saved for further analysis or submission.
