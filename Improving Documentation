# Improving AUROC of XGBoost/LightGBM in Credit Risk Scoring

Credit risk scoring has seen extensive use of **gradient boosting models** like **XGBoost** and **LightGBM**. In recent years (last ~5 years), researchers have proposed various enhancements to boost the **Area Under the ROC Curve (AUROC)** performance of these models on credit risk data (often large datasets with dozens to hundreds of features). Below, we summarize key peer-reviewed studies and their methods – focusing on credit scoring contexts – and highlight how each approach improved model AUROC. A comparison table is also provided for an at-a-glance overview.

## Feature Engineering & Data Preprocessing Techniques

**1. Dimensionality Reduction + Resampling:** *Chang Yu et al. (2024)* applied **Principal Component Analysis (PCA)** to reduce feature dimensionality and **SMOTEENN** (a combination of oversampling and cleaning under-sampling) to tackle class imbalance, before training LightGBM and XGBoost ([Advanced User Credit Risk Prediction Model using LightGBM, XGBoost and Tabnet with SMOTEENN](https://arxiv.org/html/2408.03497v1#:~:text=various%20dimensionality%20reduction%20techniques%20such,performance%20compared%20to%20other%20models)) ([Advanced User Credit Risk Prediction Model using LightGBM, XGBoost and Tabnet with SMOTEENN](https://arxiv.org/html/2408.03497v1#:~:text=By%20comparing%20the%20results%20across,the%20given%20dataset%20and%20task)). Using a bank credit dataset (~40k records), they found **LightGBM** with PCA+SMOTEENN achieved an **AUC ~0.9999**, vastly outperforming the same model on raw data (AUC ~0.63) ([Advanced User Credit Risk Prediction Model using LightGBM, XGBoost and Tabnet with SMOTEENN](https://arxiv.org/html/2408.03497v1#:~:text=By%20comparing%20the%20results%20across,the%20given%20dataset%20and%20task)). This demonstrates that carefully **preprocessing high-dimensional, imbalanced data** can dramatically improve AUROC in credit default predictions.

**2. Automated Feature Synthesis:** *Tambunan et al. (2024)* explored **Deep Feature Synthesis (DFS)** to automatically generate additional features from a peer-to-peer lending credit dataset (74 base attributes, ~887k samples) ([
		Enhancing Credit Risk Classification Using LightGBM with Deep Feature Synthesis
							| Journal of Information Systems and Informatics
			](https://journal-isi.org/index.php/isi/article/view/902#:~:text=The%20data%20used%20in%20this,81)). LightGBM models were trained with and without DFS-generated features. **DFS boosted performance**: for example, accuracy rose from 0.97 to 0.99, and F1-score from 0.81 to 0.96 with DFS ([
		Enhancing Credit Risk Classification Using LightGBM with Deep Feature Synthesis
							| Journal of Information Systems and Informatics
			](https://journal-isi.org/index.php/isi/article/view/902#:~:text=aiming%20to%20optimize%20credit%20risk,making%20it%20a%20valuable%20approach)). Although AUROC isn’t directly reported in their abstract, the large gains in recall (0.68 → 0.94) and F1 imply a significantly better ranking of good vs. bad borrowers. This indicates that **automated feature engineering** can improve discriminatory power (and likely AUROC) for boosting models in credit scoring.

**3. Hybrid Feature Selection & Balancing:** *Chen et al. (2023)* proposed a comprehensive approach to address **high-dimensional features and class imbalance** in credit risk modeling ([Research on Credit Risk Prediction under Unbalanced Dataset Based on Ensemble Learning](https://ideas.repec.org/a/hin/jnlmpe/2927393.html#:~:text=Aiming%20at%20the%20problem%20of,the%20boundary%2C%20and%20a%20new)) ([Research on Credit Risk Prediction under Unbalanced Dataset Based on Ensemble Learning](https://ideas.repec.org/a/hin/jnlmpe/2927393.html#:~:text=interpolation%20method%20is%20used%20to,show%20that%20this%20method%20effectively)). They first performed **feature selection** using an improved Relief algorithm + Max-Information Coefficient to filter features, followed by Random Forest to further reduce dimensionality ([Research on Credit Risk Prediction under Unbalanced Dataset Based on Ensemble Learning](https://ideas.repec.org/a/hin/jnlmpe/2927393.html#:~:text=method%20based%20on%20integrated%20learning,the%20new%20sample%20distribution%20more)). For imbalance, they introduced an **adaptive Borderline-SMOTE** technique (generating new minority samples with a more realistic distribution) ([Research on Credit Risk Prediction under Unbalanced Dataset Based on Ensemble Learning](https://ideas.repec.org/a/hin/jnlmpe/2927393.html#:~:text=coefficient%20to%20eliminate%20redundant%20features%2C,classify%20samples%2C%20and)). Additionally, they modified LightGBM’s loss function with **Focal Loss** (with tunable parameters α, γ) to make the model focus on minority and hard-to-classify cases ([Research on Credit Risk Prediction under Unbalanced Dataset Based on Ensemble Learning](https://ideas.repec.org/a/hin/jnlmpe/2927393.html#:~:text=Borderline,and%20random%20subspace%20methods%20to)). Finally, they **ensembled** the improved LightGBM as a base learner using AdaBoost and random subspace methods ([Research on Credit Risk Prediction under Unbalanced Dataset Based on Ensemble Learning](https://ideas.repec.org/a/hin/jnlmpe/2927393.html#:~:text=pays%20more%20attention%20to%20minority,a%20better%20default%20prediction%20effect)). In experiments, this integrated method yielded **higher G-mean and AUC** than other methods, significantly improving default prediction performance ([Research on Credit Risk Prediction under Unbalanced Dataset Based on Ensemble Learning](https://ideas.repec.org/a/hin/jnlmpe/2927393.html#:~:text=the%20base%20classifier%20and%20then,a%20better%20default%20prediction%20effect)). This study shows that combining *feature engineering*, *imbalance handling,* and *custom loss functions* can notably enhance AUROC for LightGBM in credit scoring.

## Hyperparameter Optimization Strategies

**4. Bayesian Hyperparameter Tuning (XGBoost):** One straightforward way to boost AUROC is to optimally tune the model’s hyperparameters. *Yotsawat et al. (2021)* developed an **XGBoost with Bayesian Optimization (XGB-BO)** for credit scoring ([(PDF) Improved credit scoring model using XGBoost with Bayesian hyper-parameter optimization](https://www.researchgate.net/publication/356684079_Improved_credit_scoring_model_using_XGBoost_with_Bayesian_hyper-parameter_optimization#:~:text=Several%20credit,on%20four%20widely%20public%20datasets)). After standard preprocessing, they used Bayesian optimization to find better hyperparameters for XGBoost, and evaluated on four public credit datasets (German credit, Australian credit, Lending Club, Polish bank loan) ([(PDF) Improved credit scoring model using XGBoost with Bayesian hyper-parameter optimization](https://www.researchgate.net/publication/356684079_Improved_credit_scoring_model_using_XGBoost_with_Bayesian_hyper-parameter_optimization#:~:text=The%20model%20comprises%20two%20steps,The%20proposed%20model)). The tuned XGBoost consistently **achieved the highest AUC** on all datasets compared to other classifiers (SVM, neural nets, Random Forest, etc.) and to XGBoost with default settings ([(PDF) Improved credit scoring model using XGBoost with Bayesian hyper-parameter optimization](https://www.researchgate.net/publication/356684079_Improved_credit_scoring_model_using_XGBoost_with_Bayesian_hyper-parameter_optimization#:~:text=ensemble%20model%2C%20XGB,th%20e%20Australian%2C%20German%2C%20and)). For instance, XGB-BO improved accuracy by ~2–4% over baseline models on several datasets ([(PDF) Improved credit scoring model using XGBoost with Bayesian hyper-parameter optimization](https://www.researchgate.net/publication/356684079_Improved_credit_scoring_model_using_XGBoost_with_Bayesian_hyper-parameter_optimization#:~:text=i,%C2%A9)) and had superior AUC, confirming that systematic **hyperparameter optimization** yields a more discriminative model ([(PDF) Improved credit scoring model using XGBoost with Bayesian hyper-parameter optimization](https://www.researchgate.net/publication/356684079_Improved_credit_scoring_model_using_XGBoost_with_Bayesian_hyper-parameter_optimization#:~:text=us%20e%20of%20tuned%20parameters)).

**5. Bayesian-Optimized XGBoost + Risk Indicators:** *Zhang et al. (2023)* (supply chain finance context) took a domain-specific feature engineering plus tuning approach. They built a **“risk indicator system”** of 26 features relevant to small business credit risk, then used **XGBoost** as the classifier with **Bayesian hyperparameter tuning** (comparing against SVM and Random Forest) ([The KS and AUC value of different model. | Download Scientific Diagram](https://www.researchgate.net/figure/The-KS-and-AUC-value-of-different-model_tbl1_355721422#:~:text=the%20first%20round%20of%20indicator,of%20the%20other%20control%20models)) ([The KS and AUC value of different model. | Download Scientific Diagram](https://www.researchgate.net/figure/The-KS-and-AUC-value-of-different-model_tbl1_355721422#:~:text=Compared%20to%20other%20individual%20models%2C,financial%20risk%20management%20and%20control)). The resulting **BO-XGBoost model** had **the highest AUC** and lowest prediction error among all models tested, indicating significantly better discriminative ability ([The KS and AUC value of different model. | Download Scientific Diagram](https://www.researchgate.net/figure/The-KS-and-AUC-value-of-different-model_tbl1_355721422#:~:text=Compared%20to%20other%20individual%20models%2C,financial%20risk%20management%20and%20control)). The authors note that the combination of an **efficient feature set and Bayesian-tuned XGBoost** could better differentiate risky vs. non-risky firms, thus *“underscoring the advantages of employing Bayesian optimization in XGBoost”* for credit default prediction ([The KS and AUC value of different model. | Download Scientific Diagram](https://www.researchgate.net/figure/The-KS-and-AUC-value-of-different-model_tbl1_355721422#:~:text=Compared%20to%20other%20individual%20models%2C,financial%20risk%20management%20and%20control)).

**6. Evolutionary/Heuristic Tuning (LightGBM):** Beyond Bayesian methods, meta-heuristics have been used to optimize boosting models. *Hou et al. (2025)* proposed an **Improved Sparrow Search Algorithm (ISSA)** to tune LightGBM hyperparameters for predicting SME credit risk in supply chain finance. The ISSA is a swarm intelligence optimizer that improved upon the standard Sparrow Search. The optimized LightGBM (“**ISSA-LightGBM**”) was compared to five other models ([An improved sparrow search algorithm optimized LightGBM ...](https://dl.acm.org/doi/10.1016/j.cam.2024.116197#:~:text=The%20experimental%20results%20show%20that,the%20other%20five%20models)). **Experimental results showed ISSA-LightGBM performed best**, with **superior AUROC** and overall prediction accuracy than the other models ([An improved sparrow search algorithm optimized LightGBM ...](https://www.sciencedirect.com/science/article/abs/pii/S0377042724004461#:~:text=,In%20addition%2C%20this)). This suggests that **evolutionary algorithms** can effectively find better parameter settings for LightGBM, leading to higher AUROC in credit risk applications (especially when the search space is large in high-dimensional data).

## Ensemble & Hybrid Modeling Techniques

**7. Ensemble of Boosting with Other Learners:** Some studies improve AUROC by combining the strengths of multiple algorithms. The earlier *Chen et al. (2023)* work already illustrated an ensemble (AdaBoost with LightGBM as base) that boosted performance ([Research on Credit Risk Prediction under Unbalanced Dataset Based on Ensemble Learning](https://ideas.repec.org/a/hin/jnlmpe/2927393.html#:~:text=pays%20more%20attention%20to%20minority,a%20better%20default%20prediction%20effect)). Another approach is to hybridize gradient boosting with neural networks. *Li et al. (2022)* introduced a **two-stage XGBoost-MLP hybrid** for supply chain credit risk. In stage 1, an XGBoost model selects important features and in stage 2, a Multi-Layer Perceptron network makes the final prediction ([(PDF) A Hybrid XGBoost-MLP Model for Credit Risk Assessment on Digital Supply Chain Finance](https://www.researchgate.net/publication/358224527_A_Hybrid_XGBoost-MLP_Model_for_Credit_Risk_Assessment_on_Digital_Supply_Chain_Finance#:~:text=Layer%20Perceptron%20%28XGBoost,of%20DSCF%2C%20the%20results%20show)). This XGBoost-MLP model showed **“good performance”** on a dataset of 1,357 observations (85 SMEs) – the authors highlight that XGBoost-based feature selection improved the MLP’s results ([(PDF) A Hybrid XGBoost-MLP Model for Credit Risk Assessment on Digital Supply Chain Finance](https://www.researchgate.net/publication/358224527_A_Hybrid_XGBoost-MLP_Model_for_Credit_Risk_Assessment_on_Digital_Supply_Chain_Finance#:~:text=Layer%20Perceptron%20%28XGBoost,of%20DSCF%2C%20the%20results%20show)). While specific AUROC values aren’t quoted in the abstract, the implication is that this ensemble benefitted from XGBoost’s ability to handle many features and the MLP’s ability to capture complex patterns. **Ensembling** different model types can thus yield a higher AUROC than either alone, provided each component addresses a different aspect of the problem.

**8. Comparative Boosting Algorithm Performance:** For completeness, it’s worth noting that not all boosting algorithms respond equally to enhancements. *Coşkun & Turanlı (2023)* evaluated XGBoost, LightGBM, and CatBoost on a large Home Credit default dataset (~92% non-default) ([Role of Interface Manipulation Style and Scaffolding on Cognition and Concept Learning in Learnware](https://sciendo.com/pdf/10.2478/jamsi-2023-0001#:~:text=4,on%20Kaggle%20Home%20Credit%20Default)). They found that after basic hyperparameter tuning, **CatBoost’s AUC improved significantly**, whereas **XGBoost and LightGBM’s AUC remained about the same** (around 0.755–0.757) ([Role of Interface Manipulation Style and Scaffolding on Cognition and Concept Learning in Learnware](https://sciendo.com/pdf/10.2478/jamsi-2023-0001#:~:text=approximately%2034.7,score%20of%20the%20Catboost%20model)). Nonetheless, XGBoost/LightGBM were already strong performers in that study ([Role of Interface Manipulation Style and Scaffolding on Cognition and Concept Learning in Learnware](https://sciendo.com/pdf/10.2478/jamsi-2023-0001#:~:text=match%20at%20L537%20the%20models,on%20Kaggle%20Home%20Credit%20Default)). This underscores that the **effectiveness of tuning or enhancements can vary by algorithm** and dataset, and one should experiment with multiple approaches.

## Comparison of Methods and Outcomes

The table below summarizes the above papers, their proposed methods for improving XGBoost/LightGBM, and the reported impact on AUROC in credit risk scoring tasks:

| **Study (Year)**                                              | **Proposed Enhancement**                                   | **Credit Risk Data**                   | **AUROC Improvement**                                             |
|---------------------------------------------------------------|------------------------------------------------------------|----------------------------------------|-------------------------------------------------------------------|
| **Yu et al., 2024 ([Advanced User Credit Risk Prediction Model using LightGBM, XGBoost and Tabnet with SMOTEENN](https://arxiv.org/html/2408.03497v1#:~:text=By%20comparing%20the%20results%20across,the%20given%20dataset%20and%20task))** (LightGBM focus)             | PCA dimensionality reduction + SMOTEENN balancing           | Bank credit card dataset (40k records) | LightGBM AUC ↑ from ~0.63 to **0.9999** with PCA+SMOTEENN ([Advanced User Credit Risk Prediction Model using LightGBM, XGBoost and Tabnet with SMOTEENN](https://arxiv.org/html/2408.03497v1#:~:text=By%20comparing%20the%20results%20across,the%20given%20dataset%20and%20task)). Best model (LightGBM) far outperformed baseline boosting models on raw data. |
| **Yotsawat et al., 2021 ([(PDF) Improved credit scoring model using XGBoost with Bayesian hyper-parameter optimization](https://www.researchgate.net/publication/356684079_Improved_credit_scoring_model_using_XGBoost_with_Bayesian_hyper-parameter_optimization#:~:text=ensemble%20model%2C%20XGB,th%20e%20Australian%2C%20German%2C%20and))** (XGBoost focus)       | Bayesian hyperparameter optimization for XGBoost (XGB-BO)   | 4 public credit datasets (German, etc.)| XGB-BO achieved **highest AUC on all datasets** versus default XGB and other classifiers ([(PDF) Improved credit scoring model using XGBoost with Bayesian hyper-parameter optimization](https://www.researchgate.net/publication/356684079_Improved_credit_scoring_model_using_XGBoost_with_Bayesian_hyper-parameter_optimization#:~:text=us%20e%20of%20tuned%20parameters)). Notably improved accuracy by ~2–4% over baseline models. |
| **Chen et al., 2023 ([Research on Credit Risk Prediction under Unbalanced Dataset Based on Ensemble Learning](https://ideas.repec.org/a/hin/jnlmpe/2927393.html#:~:text=pays%20more%20attention%20to%20minority,a%20better%20default%20prediction%20effect))** (LightGBM ensemble)         | Feature selection (Relief/MIC/RF) + adaptive Borderline-SMOTE + LightGBM with focal loss, ensembled via AdaBoost & subspace | Loan default data (high-dim, imbalanced) | **Significantly higher AUC** (and G-mean) than other methods ([Research on Credit Risk Prediction under Unbalanced Dataset Based on Ensemble Learning](https://ideas.repec.org/a/hin/jnlmpe/2927393.html#:~:text=the%20base%20classifier%20and%20then,a%20better%20default%20prediction%20effect)). The integrated approach yielded better default discrimination than standard boosting or sampling alone. |
| **Zhang et al., 2023 ([The KS and AUC value of different model. | Download Scientific Diagram](https://www.researchgate.net/figure/The-KS-and-AUC-value-of-different-model_tbl1_355721422#:~:text=Compared%20to%20other%20individual%20models%2C,financial%20risk%20management%20and%20control))** (XGBoost focus)          | Engineered “risk indicator” features + Bayesian-tuned XGBoost (BO-XGBoost) | SME supply-chain finance credit data   | **Highest AUC** among models (vs. SVM, RF) ([The KS and AUC value of different model. | Download Scientific Diagram](https://www.researchgate.net/figure/The-KS-and-AUC-value-of-different-model_tbl1_355721422#:~:text=Compared%20to%20other%20individual%20models%2C,financial%20risk%20management%20and%20control)). BO-XGBoost showed superior discriminative power, reducing prediction errors and default misclassification. |
| **Hou et al., 2025 ([An improved sparrow search algorithm optimized LightGBM ...](https://www.sciencedirect.com/science/article/abs/pii/S0377042724004461#:~:text=,In%20addition%2C%20this))** (LightGBM focus)                | Improved Sparrow Search Algorithm (ISSA) for LightGBM tuning| SME supply-chain finance credit data   | **Outperformed 5 other models** in AUROC and accuracy ([An improved sparrow search algorithm optimized LightGBM ...](https://dl.acm.org/doi/10.1016/j.cam.2024.116197#:~:text=The%20experimental%20results%20show%20that,the%20other%20five%20models)). ISSA-optimized LightGBM demonstrated superior credit risk prediction capability (vs. standard tuning). |
| **Tambunan et al., 2024 ([
		Enhancing Credit Risk Classification Using LightGBM with Deep Feature Synthesis
							| Journal of Information Systems and Informatics
			](https://journal-isi.org/index.php/isi/article/view/902#:~:text=aiming%20to%20optimize%20credit%20risk,making%20it%20a%20valuable%20approach))** (LightGBM focus)        | Deep Feature Synthesis (automated feature engineering)      | P2P lending dataset (74→many features) | **Higher performance metrics** with DFS (Accuracy 0.99 vs 0.97) ([
		Enhancing Credit Risk Classification Using LightGBM with Deep Feature Synthesis
							| Journal of Information Systems and Informatics
			](https://journal-isi.org/index.php/isi/article/view/902#:~:text=aiming%20to%20optimize%20credit%20risk,making%20it%20a%20valuable%20approach)). Implies a notable AUROC gain by capturing additional predictive factors. |

**Sources:** Summaries based on findings from peer-reviewed articles and conference papers, with citations to the original works for detailed results. All the above studies highlight that **tailoring the modeling pipeline** – through smarter features, balancing techniques, tuning algorithms, or hybrid ensembles – can yield measurable improvements in AUROC for credit risk scoring models like XGBoost and LightGBM.

